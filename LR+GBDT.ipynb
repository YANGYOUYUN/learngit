{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time时间戳: 1616221464.080758\n",
      "tmp_wax_1001_model_flag_xn_yx_jg_c 表记录数: 300000\n",
      "\n",
      "\n",
      "数据导出前时间戳: 1616221466.0305872 累计耗时： 1.9498291015625 区间耗时： 0.9983372688293457\n",
      "\n",
      "\n",
      "用DataFrame结构存储数据，以便数据操作和建模(这个时间较久1)\n",
      "数据导出后时间戳: 1616222023.2481108 累计耗时： 559.1673526763916 区间耗时： 557.2175235748291\n",
      "\n",
      "\n",
      "['os', 'agerange', 'gender', 'car', 'house', 'child', 'position', 'love', 'educate', 'interest_101201', 'interest_101103', 'interest_101102', 'interest_101101', 'interest_101004', 'interest_101003', 'interest_101002', 'interest_101001', 'interest_100102', 'interest_100101', 'interest_101902', 'interest_101804', 'interest_101805', 'interest_101903', 'interest_101801', 'interest_101803', 'interest_101901', 'interest_101704', 'interest_101802', 'interest_101701', 'interest_101702', 'interest_101703', 'interest_101601', 'interest_101603', 'interest_101604', 'interest_101503', 'interest_101504', 'interest_101602', 'interest_10160x', 'interest_101501', 'interest_101502', 'interest_10150x', 'interest_101404', 'interest_101405', 'interest_101406', 'interest_101401', 'interest_101402', 'interest_101403', 'interest_101303', 'interest_101304', 'interest_101205', 'interest_101301', 'interest_101302', 'interest_101204', 'interest_101206', 'interest_101207', 'interest_101105', 'interest_101202', 'interest_101203', 'interest_101104', 'interest_100903', 'interest_100904', 'interest_100803', 'interest_100901', 'interest_100902', 'interest_100801', 'interest_100802', 'interest_100701', 'interest_100702', 'interest_100703', 'interest_100601', 'interest_100602', 'interest_100603', 'interest_10060x', 'interest_100502', 'interest_100503', 'interest_100504', 'interest_100404', 'interest_100501', 'interest_100401', 'interest_100402', 'interest_100403', 'interest_100301', 'interest_100302', 'interest_100303', 'interest_10030x', 'interest_100204', 'interest_100205', 'interest_100206', 'interest_100202', 'interest_100203', 'interest_100201', 'app_11100', 'app_11000', 'app_10900', 'app_10800', 'app_10700', 'app_10600', 'app_10500', 'app_10400', 'app_10100', 'app_10300', 'app_10200', 'app_20000', 'province_area', 'city_level', 'model_price_cate', 'moax_user_flag', 'tsax_user_flag', 'tax_user_flag', 'moax_last30d_login_d', 'tsax_last30d_login_d', 'tax_last30d_login_d', 'fufei_flag', 'dzdp_act_user', 'gzvip_user', 'douyin_view_user', 'huoshan_view_user', 'xigua_view_user', 'tax_adx_user', 'qqkongjian_user', 'tengxun_xw_user', 'guanggao_lm_user', 'app_31001004', 'app_31001007', 'app_31001015', 'app_31001012', 'app_31001018', 'app_31001022', 'app_31001016', 'app_31001008', 'app_31001009', 'app_31001025', 'app_31001011', 'app_31001013', 'app_31001017', 'app_31001020', 'app_31001023', 'app_31001024', 'app_31001002', 'app_31001003', 'app_31001005', 'app_31001014', 'app_31001021', 'app_31001001', 'app_31001006', 'app_31001010', 'app_31001019', 'app_31002004', 'app_31002010', 'app_31002014', 'app_31002008', 'app_31002001', 'app_31002002', 'app_31002011', 'app_31002006', 'app_31002009', 'app_31002016', 'app_31002015', 'app_31002005', 'app_31002012', 'app_31002013', 'app_31002003', 'app_31002007', 'life_160030102', 'life_170010104', 'life_170030101', 'login_210000', 'login_210001', 'login_210002', 'login_210003', 'network_60110100', 'network_60110200', 'network_60110302', 'network_60110303', 'network_60110304', 'life_170020101', 'life_170020102', 'life_170020103', 'life_170020104', 'high_value_user', 'high_zhuanhua_user']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import lightgbm as lgb \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "#coding:utf8\n",
    "import time\n",
    "time0=time.time()\n",
    "print('start_time时间戳:',time0)\n",
    "from importlib import reload\n",
    "from odps import ODPS\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder                 #分类变量数字化编码\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier            #随机森林\n",
    "from sklearn.ensemble import GradientBoostingClassifier        #梯度上升\n",
    "from sklearn.model_selection import train_test_split          #数据拆分\n",
    "from sklearn.linear_model import LogisticRegression as LR      #逻辑回归\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import roc_curve,auc                      #模型效果评估\n",
    "from odps.df import DataFrame                                  #odps表数据读取\n",
    "import information_value2 as iv                                 #导入iv/woe计算包\n",
    "reload(iv)\n",
    "from scipy import stats                                        #描述统计包\n",
    "from sklearn.feature_selection import SelectFromModel          #模型特征选择\n",
    "from odps.utils import init_progress_ui\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "time1=time.time()\n",
    "# # 读取数据\n",
    "# # def preProcess():\n",
    "# path = 'data/'\n",
    "# df_train = pd.read_csv(path + 'train.csv')\n",
    "# df_test = pd.read_csv(path + 'test.csv')\n",
    "# df_train.drop(['Id'], axis = 1, inplace = True)\n",
    "# df_test.drop(['Id'], axis = 1, inplace = True)\n",
    "# df_test['Label'] = -1\n",
    "# data = pd.concat([df_train, df_test])\n",
    "# data = data.fillna(-1)\n",
    "# data.to_csv('data/data.csv', index = False)\n",
    "# #     return data\n",
    "\n",
    "\n",
    "def load_data(table_name):\n",
    "    \"\"\"加载数据集,数据预处理\"\"\"\n",
    "\n",
    "    odps = ODPS(\"LTAI4GAgxSUBsvhqBpkwP4eR\",\"Wjbt6SvdgH3vpJ925lp74DUQMkwWkL\",\"wead\")    #odps对象创建（连接设置）\n",
    "    project = odps.get_project('wead')\n",
    "#     odps_table = 'tmp_wax_1001_model_exp_act_kmyx_rmj_i' # 表名\n",
    "    odps_table = table_name\n",
    "    t = odps.get_table(odps_table)     # 获取某个表\n",
    "    readers = t.open_reader()   # 读取表reader = t.open_reader(partition='pt=test')\n",
    "    count = readers.count       # 读取表记录数\n",
    "    print (u'%s 表记录数: %s'  %(odps_table,count))\n",
    "\n",
    "\n",
    "\n",
    "    dd=pd.DataFrame()            #生成空表\n",
    "    dd['name']=t.schema.names    #往表中加变量名称\n",
    "    dd['type']=t.schema.types    #往表中加变量类型\n",
    "    cc=dd[(dd['name']!='user_id') & (dd['name']!='p_date') & (dd['name']!='model_class') & (dd['name']!='user_veid') & (dd['name']!='is_act')& (dd['name']!='randnum')& (dd['name']!='rn1')&(dd['name']!='rn')]      #只取自变量名称和类型\n",
    "    t_var_name = t.schema.names         #获取所有变量名称列表\n",
    "    z_var_name=list(cc['name'])         #获取自变量名称列表 \n",
    "    y_var_name='is_act'            #获取因变量名称\n",
    "    u_var_name=t_var_name[0]                            #获取用户id\n",
    "    s_var_name=list(cc['name'][cc['type']=='string'])   #获取分类变量名称\n",
    "    v_var_name=list(cc['name'][cc['type']!='string'])   #获取数值变量名称\n",
    "\n",
    "\n",
    "    time2=time.time()\n",
    "    print('\\n')\n",
    "    print('数据导出前时间戳:',time2,'累计耗时：',time2-time0,'区间耗时：',time2-time1)\n",
    "\n",
    "    print('\\n')\n",
    "    print ( u'用DataFrame结构存储数据，以便数据操作和建模(这个时间较久1)')\n",
    "    ui = init_progress_ui(mock=True) \n",
    "    df1 = DataFrame(t).to_pandas(ui=ui)  \n",
    "\n",
    "    time3=time.time()\n",
    "    print('数据导出后时间戳:',time3,'累计耗时：',time3-time0,'区间耗时：',time3-time2)\n",
    "    print('\\n')\n",
    "    return df1,s_var_name,v_var_name,z_var_name,y_var_name\n",
    "\n",
    "\n",
    "df1,s_var_name,v_var_name,z_var_name,y_var_name = load_data('tmp_wax_1001_model_flag_xn_yx_jg_c')\n",
    "\n",
    "print(z_var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \n",
    "     #缺失值填充\n",
    "    df1[s_var_name]=df1[s_var_name].fillna(value='未知')                 #（分类）自变量缺失值填充\n",
    "    df1[v_var_name]=df1[v_var_name].fillna(df1[v_var_name].mean())       #数值自变量以均值填充\n",
    "\n",
    "    time4=time.time()\n",
    "#     print('\\n')\n",
    "#     print('数据缺失值填充时间戳:',time4,'累计耗时：',time4-time0,'区间耗时：',time4-time3)\n",
    "\n",
    "    #各变量缺失值替换值\n",
    "    default_var = []   \n",
    "    for var in z_var_name:\n",
    "        if var in s_var_name:\n",
    "            default_var.append('未知')\n",
    "        else:\n",
    "            default_var.append(df1[var].mean())\n",
    "\n",
    "    s_default_var=pd.Series(default_var,index=z_var_name)\n",
    "    \n",
    "    \n",
    "    # 对分类自变量进行编码\n",
    "    z_var_dict={}\n",
    "    for var in z_var_name:     \n",
    "        num = LabelEncoder()\n",
    "        df1[var] = num.fit_transform(df1[var].astype('str'))\n",
    "        dict0={}\n",
    "        for label in num.classes_:\n",
    "            dict0[label]=num.transform([label])[0]\n",
    "        z_var_dict[var]=dict0\n",
    "\n",
    "    time5=time.time()\n",
    "    print('数据缺失值保存时间戳:',time5,'累计耗时：',time5-time0,'区间耗时：',time5-time4)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # 变量分段\n",
    "    # 对数值变量进行分箱\n",
    "    df2=df1.copy()\n",
    "    iva=iv.WOE()\n",
    "    x_cate_max_rate_dict,x_cate_max_rate_array=iva.x_cate_max_rate(df2[z_var_name], df2[y_var_name], event=1)\n",
    "    z_var_name0=list(np.array(z_var_name)[x_cate_max_rate_array>0.9])\n",
    "    z_var_name1=list(set(z_var_name)-set(z_var_name0))  \n",
    "    v_var_name1=list(set(z_var_name1).intersection(set(v_var_name)))  \n",
    "    s_var_name1=list(set(z_var_name1).intersection(set(s_var_name)))  \n",
    "\n",
    "\n",
    "    df2[v_var_name1],v_var_dicrete=iva.feature_discretion(df2[v_var_name1])            #数值变量分箱并替换\n",
    "    v_dict=dict(zip(v_var_name1,v_var_dicrete))                                       #数值变量分段标准字典\n",
    "\n",
    "    time6=time.time()\n",
    "    print('\\n')\n",
    "    print('对数值变量分箱时间戳:',time6,'累计耗时：',time6-time0,'区间耗时：',time6-time5)\n",
    "    # 对分类变量进行分箱\n",
    "    #计算分类变量iv值及woe，对变量进行WOE转换后再进行分箱处理\n",
    "    w_dict,i_dict,w_list,i_list=iva.woe(df2[s_var_name1], df2[y_var_name], event=1)    #(这个时间较久)\n",
    "\n",
    "    print('\\n')\n",
    "    print ( u'计算分类变量iv值及woe，对变量进行WOE转换后再进行分箱处理(这个时间较久2)')\n",
    "    df2[s_var_name1]=iva.woe_replace(df2[s_var_name1],w_list)                    #对变量进行WOE转换     #(这个时间较久)\n",
    "    df2[s_var_name1],s_var_dicrete=iva.feature_discretion(df2[s_var_name1])      #分类变量分箱并替换\n",
    "    s_dict=dict(zip(s_var_name1,s_var_dicrete))               #分类变量分段标准字典       \n",
    "    s_var_dicrete_dict=iva.exchage_dict(s_dict,w_dict)       #分类变量分段映射关系字典\n",
    "    # 合并分类变量与数值变量的分箱数据\n",
    "    z_var_dicrete_dict=s_var_dicrete_dict.copy()\n",
    "    z_var_dicrete_dict.update(v_dict)                        \n",
    "\n",
    "    time7=time.time()\n",
    "    #print('对分类变量分箱时间戳:',time7,'累计耗时：',time7-time0,'区间耗时：',time7-time6)\n",
    "    # 对分箱后的数据进行数据类型转换\n",
    "    for var in z_var_name1:\n",
    "        df2[var] = df2[var].astype(str)\n",
    "    time8=time.time()\n",
    "    #print('对分箱后的数据进行str转换时间戳:',time8,'累计耗时：',time8-time0,'区间耗时：',time8-time7)\n",
    "    #训练集/测试集划分\n",
    "#     point1= stats.scoreatpercentile(df2['p_date'], 70)  \n",
    "#     point2= stats.scoreatpercentile(df2['p_date'], 75) \n",
    "#     if  point1==point2:\n",
    "    x_train,x_test, y_train, y_test = train_test_split(df2[z_var_name1], df2[y_var_name], test_size = 0.3, random_state=2020)      #默认按1:3划分训练和测试样本\n",
    "    print (u'按比例随机划分训练集/测试集')\n",
    "#     else:\n",
    "#         df_train=df2[df2['p_date']<=point1]\n",
    "#         df_test=df2[df2['p_date']>point1]\n",
    "#         x_train=df_train[z_var_name1]\n",
    "#         x_test=df_test[z_var_name1]\n",
    "#         y_train=df_train[y_var_name]\n",
    "#         y_test=df_test[y_var_name]\n",
    "#         print (u'按时间先后划分训练集/测试集')\n",
    "#     print('\\n')\n",
    "#     print('-' * 60)\n",
    "#     print ('point1:' , point1)\n",
    "#     print('-' * 60)\n",
    "\n",
    "    time9=time.time()\n",
    "    print('\\n')\n",
    "    print('对数据进行划分数据集时间戳:',time9,'累计耗时：',time9-time0,'区间耗时：',time9-time8)\n",
    "    #对分箱后的所有自变量计算iv值及woe，并进行WOE转换，再根据iv值进行变量预筛选\n",
    "    w_dict1,i_dict1,w_list1,i_list1=iva.woe(df2[z_var_name1], df2[y_var_name], event=1)     #(这个时间较久)\n",
    "\n",
    "    time10=time.time()\n",
    "\n",
    "    z_var_woe_dict=iva.exchage_func(z_var_dicrete_dict,w_dict1)      #变量各段类与对应WOE的映射关系字典\n",
    "\n",
    "    # 对变量进行WOE转换\n",
    "    x_train[z_var_name1]=iva.woe_replace(x_train[z_var_name1],w_list1)\n",
    "    x_test[z_var_name1]=iva.woe_replace(x_test[z_var_name1],w_list1)\n",
    "\n",
    "    time11=time.time()\n",
    "\n",
    "\n",
    "    # 根据iv值筛选入模训练变量\n",
    "    n_features = 15\n",
    "    iv_df=pd.Series(i_list1,index=x_train.columns)\n",
    "    s_iv_df=iv_df.nlargest(n_features)\n",
    "    s_iv_name=list(s_iv_df.index)\n",
    "    print('-' * 60)\n",
    "    print(u'前%d个s_iv_df'%n_features)\n",
    "    print(s_iv_df)\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #     类别特征one-hot编码\n",
    "    for col in s_var_name:\n",
    "        onehot_feats = pd.get_dummies(df2[col], prefix = col)\n",
    "        df2.drop([col], axis = 1, inplace = True)\n",
    "        df2 = pd.concat([df2, onehot_feats], axis = 1)\n",
    "\n",
    "    x_train,x_val,y_train,y_val = train_test_split(df2,df2[y_var_name],test_size=0.3,random_state=2021)\n",
    "    print(x_train.head())\n",
    "    return x_train,x_val,y_train,y_val,s_iv_name\n",
    "    \n",
    "    \n",
    "x_train,x_val,y_train,y_val,s_iv_name = preprocess()\n",
    "data = x_train\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbdt_lr_predict(data): \n",
    "\n",
    "    n_estimators = 50\n",
    "    num_leaves = 64\n",
    "    # 开始训练gbdt，使用100课树，每课树64个叶节点\n",
    "    model = lgb.LGBMRegressor(objective='binary',\n",
    "                            subsample= 0.8,\n",
    "                            min_child_weight= 0.5,\n",
    "                            colsample_bytree= 0.7,\n",
    "                            num_leaves=num_leaves,\n",
    "                            learning_rate=0.05,\n",
    "                            n_estimators=n_estimators,\n",
    "                            random_state = 2020)\n",
    "    model.fit(x_train, y_train,\n",
    "            eval_set = [(x_train, y_train), (x_val, y_val)],\n",
    "            eval_names = ['train', 'val'],\n",
    "            eval_metric = 'binary_logloss',\n",
    "            verbose=0)\n",
    "    \n",
    "    # 得到每一条训练数据落在了每棵树的哪个叶子结点上\n",
    "    # pred_leaf = True 表示返回每棵树的叶节点序号\n",
    "    gbdt_feats_train = model.predict(train, pred_leaf = True)\n",
    "    \n",
    "    # 打印结果的 shape：\n",
    "    print(gbdt_feats_train.shape)\n",
    "    # 打印前5个数据：\n",
    "    print(gbdt_feats_train[:5])\n",
    "    \n",
    "    # 同样要获取测试集的叶节点索引\n",
    "    gbdt_feats_test = model.predict(test, pred_leaf = True)\n",
    "    \n",
    "    # 将 32 课树的叶节点序号构造成 DataFrame，方便后续进行 one-hot\n",
    "    gbdt_feats_name = ['gbdt_leaf_' + str(i) for i in range(n_estimators)]\n",
    "    df_train_gbdt_feats = pd.DataFrame(gbdt_feats_train, columns = gbdt_feats_name) \n",
    "    df_test_gbdt_feats = pd.DataFrame(gbdt_feats_test, columns = gbdt_feats_name)\n",
    "    train_len = df_train_gbdt_feats.shape[0]\n",
    "    data = pd.concat([df_train_gbdt_feats, df_test_gbdt_feats])\n",
    "\n",
    "    # 对每棵树的叶节点序号进行 one-hot\n",
    "    for col in gbdt_feats_name:\n",
    "        onehot_feats = pd.get_dummies(data[col], prefix = col)\n",
    "        data.drop([col], axis = 1, inplace = True)\n",
    "        data = pd.concat([data, onehot_feats], axis = 1)\n",
    "\n",
    "    train = data[: train_len]\n",
    "    test = data[train_len:]\n",
    "    \n",
    "    # 划分 LR 训练集、验证集\n",
    "    x_train, x_val, y_train, y_val = train_test_split(train, target, test_size = 0.3, random_state = 2018)\n",
    "    \n",
    "    # 开始训练lr\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    tr_logloss = log_loss(y_train, lr.predict_proba(x_train)[:, 1])\n",
    "    tr_auc = roc_auc_score(y_train,lr.predict_proba(x_train)[:,1])\n",
    "#     print('tr-logloss: ', tr_logloss)\n",
    "    print('train_auc:',tr_auc)\n",
    "    val_logloss = log_loss(y_val, lr.predict_proba(x_val)[:, 1])\n",
    "    test_auc  = roc_auc_score(y_val,lr.predict_proba(x_val)[:,1])\n",
    "#     print('val-logloss: ', val_logloss)\n",
    "    print('test_auc:',test_auc)\n",
    "    # 对测试集预测\n",
    "    y_pred = lr.predict_proba(test)[:, 1]\n",
    "#     test_val = pd.concat(df_test['Id'],y_ped)\n",
    "    print(len(y_pred))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     data = preProcess()\n",
    "#     continuous_feature = ['I'] * 13\n",
    "#     continuous_feature = [col + str(i + 1) for i, col in enumerate(continuous_feature)] \n",
    "#     category_feature = ['C'] * 26\n",
    "#     category_feature = [col + str(i + 1) for i, col in enumerate(category_feature)] \n",
    "#     gbdt_lr_predict(data, category_feature, continuous_feature)\n",
    "#     print(continuous_feature,category_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_lr_predict(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
